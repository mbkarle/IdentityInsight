# IDENTITYINSIGHT DESIGN

## Overall Design Goals

Our project was designed to be used by site impaired people to identify the names of the people in front of them. Our original design goals included forming a database of names and faces from facebook. After talking with Luke's TF (Channy Hong), we changed our goals due to privacy concerns. Instead we wanted to make a proof of concept app with a database of set names and faces of people whom we had received permission to use their images. This meant our new "BEST" objective was to create a seamless full stack project with integrated server and application design. To this end, we changed another one of our design goals. Initially we wanted to use video input, but we realised to create a more seamless experience, single photos would be better due to the processing time required to identify all the faces in a stream of video. This meant our overall design goal for the project was to produce a proof of concept application. We wanted the features of this application to be as follows. It would allow the user to take a photo, this photo to be sent to a server backend that processed it using python face-recognition code. The server would then send back the names of the people it identified in the photo (from the database of names and faces that it has). The app would then audibly read out the names of the faces that it recognised. In the end we were able to implement all these features.

## Front End

The front end implementation of IdentityInsight is an iOS application. We made it very simple so as to be easy to use. It has a central `UIImageView` that displays the photo that was taken. It also has a button labeled "Take Photo". Upon pressing the button, a `UIImagePickerController` is created. This is what we used to allow the user to take a photo using the inbuilt iOS camera software. Once the photo has been taken, the `UIImageView` is updated with the photo and our `uploadImage` function is called. To upload the image to our AWS server backend, we used a package called `Alamofire`. This strealines making http requests. We set up variables containing the URL of our server and the JPEG data for the image taken from the camera (converted from `UIImage` data type). The following code consists of an alamofire upload request. The response from the server contains the names of the people that the face-recognition software found. We take these names and input them into some iOS text to speech functions contained within the `AVFoundation`library.

## Backend

The backend implementation makes use of a NodeJS server hosted on an AWS EC2 instance. We rented a t2.medium instance from Amazon Web Servers and used secure shell terminal interfaces to download our github repository onto the machine. We designed the server in Node due to the ease afforded by the Express package, which allowed us to easily create a server and listen on port 3000. We used additional middleware with the server such as `body-parser` and `Multer` to parse request data and upload received images instantly to our `captured_frames` folder. We defined one main endpoint for our server, a post request to `/recognize`. This would invoke Multer's downloading functionality and spawn a command line call to our face recognition script. The python script makes use of facial recognition technology to check faces detected in captured_frames against known faces found in the images folder. It then sends a response with the names it has found, which is handled by the front end.

